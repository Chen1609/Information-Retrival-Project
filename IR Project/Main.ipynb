{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file 2000 pages\n",
    "from assignment 2, working on small dataset, most of the pages are not the same as provided in\n",
    "queries_train.json for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "pkl_file = \"part15_preprocessed.pkl\"\n",
    "try:\n",
    "    if os.environ[\"assignment_2_data\"] is not None:\n",
    "      pkl_file = Path(os.environ[\"assignment_2_data\"])\n",
    "except:\n",
    "   Exception(\"Problem with one of the variables\")\n",
    "   \n",
    "assert os.path.exists(pkl_file), 'You must upload this file.'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "  pages = pickle.load(f)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Wikipedia Page Title\n",
    "with given pages ids (list), the function generate a new list of the titles for each page id\n",
    "not all pages exist, or for some reason doesn't have title, so we catch the error there, no bigi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "def get_page_titles(pagesId):\n",
    "  ''' Returns the title of the first, fourth, and fifth pages as ranked about \n",
    "      by PageRank.\n",
    "  Returns:\n",
    "  --------\n",
    "    list of three strings.\n",
    "  '''\n",
    "  # YOUR CODE HERE\n",
    "  def get_pageId_title(pageID):\n",
    "    url_json_wiki = lambda x : f\"https://en.wikipedia.org/w/api.php?action=query&inprop=url&pageids={x}&prop=info&format=json\"   \n",
    "    # store the response of URL\n",
    "    response = urlopen(url_json_wiki(pageID)) \n",
    "    # storing the JSON response \n",
    "    # from url in data\n",
    "    data_json = json.loads(response.read())\n",
    "    try:\n",
    "      return data_json['query']['pages'][str(pageID)]['title']\n",
    "    except:\n",
    "      return pageID\n",
    "    \n",
    "\n",
    "  titles = [get_pageId_title(pageId) for pageId in pagesId]\n",
    "  return titles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of indexes\n",
    "creating indexes for:\n",
    "- Body\n",
    "- Title\n",
    "- anchor\n",
    "\n",
    "docs a set for all docs in our index, would be used later for intersection with pageViews to save only the relvent files for our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Parser import process_wiki\n",
    "body_idx, title_idx, anchor_idx, docs = process_wiki(pages, 'all_words')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index import InvertedIndex\n",
    "\n",
    "body_idx2 = InvertedIndex.read_index('body_indices', 'all_words')\n",
    "# title_idx = InvertedIndex.read_index('title_index', 'all_words')\n",
    "# anchor_idx = InvertedIndex.read_index('anchor_index', 'all_words')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "an basic tokenizer, later on will add classification of:\n",
    "- Bold words, need to understand if its an html or wikimedia syntax\n",
    "- Dates? years?\n",
    "- U.S.A == USA to get consistent and reasonable queries\n",
    "\n",
    "and more...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "stopwords_frozen = frozenset(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string , represting the text to tokenize.    \n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    list of tokens (e.g., list of tokens).\n",
    "    \"\"\"\n",
    "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
    "    return list_of_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "Given Queries and relevant Documents for each query.\n",
    "we load and later on will test the Measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('queries_train.json') as f:\n",
    "    data = json.load(f)\n",
    "# (q_id : (query_text, [relvent docs_ids]))\n",
    "test_data = {q_id: d for q_id, d in enumerate(data.items())}\n",
    "queries = {q_id: qf[0] for q_id, qf in test_data.items()}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Functions \n",
    "So far done:\n",
    "#### Minimum Requierments:\n",
    "- Cosine Similarity on body\n",
    "- Binary Similarity on title\n",
    "- Binary Similarity on anchor\n",
    "- Page Views ranked \n",
    "\n",
    "### TODO\n",
    "- PageRank, assignment 3 contain the GraphQL implementation\n",
    "- Build adapters for all functions to work using PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import Search\n",
    "from cosineSimilarity import Cosine_Similarity\n",
    "from binarySimilarity import Binary_Similarity\n",
    "\n",
    "queries_tokenize = {q_id: tokenize(q) for q_id, q in queries.items()}\n",
    "search = Search()\n",
    "tfidf_queries_score_train_bsS_title = search.get_topN_score_for_queries(queries_tokenize, title_idx, N=30, score=Binary_Similarity)\n",
    "tfidf_queries_score_train_bsS_anchor = search.get_topN_score_for_queries(queries_tokenize, anchor_idx, N=30, score=Binary_Similarity)\n",
    "tfidf_queries_score_train_cosS_body = search.get_topN_score_for_queries(queries_tokenize, body_idx, N=30, score=Cosine_Similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track and don't Delete previous Datasets, Measure the Engine Improvments\n",
    "### Writing the results from Similarity function\n",
    "Writing to Excel files on different sheets the results\n",
    "!!! Warnning !!! take alot of time to run so next box offer faster display\n",
    "but inconvinent. (15 minutes N=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.ExcelWriter(\"Output.xlsx\") as writer:\n",
    "    pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_bsS_title.items()}, \\\n",
    "     orient='index').to_excel(writer, sheet_name='Binary_Similarity_title', engine='xlsxwriter')\n",
    "    pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_bsS_anchor.items()}, \\\n",
    "     orient='index').to_excel(writer, sheet_name='Binary_Similarity_anchor', engine='xlsxwriter')\n",
    "    pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_cosS_body.items()}, \\\n",
    "     orient='index').to_excel(writer, sheet_name='Cosine_Similarity_body', engine='xlsxwriter')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to CSV\n",
    "Much faster then Writing to an Excel File, but hard to read and understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "dirname = os.path.join(os.getcwd(), 'Data')\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_bsS_title.items()}, \\\n",
    "    orient='index').to_csv(path_or_buf=os.path.join(dirname, 'Binary_Similarity_title.csv'), index=False)\n",
    "pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_bsS_anchor.items()}, \\\n",
    "    orient='index').to_csv(path_or_buf=os.path.join(dirname, 'Binary_Similarity_anchor.csv'), index=False)\n",
    "pd.DataFrame.from_dict({queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_cosS_body.items()}, \\\n",
    "    orient='index').to_csv(path_or_buf=os.path.join(dirname, 'Cosine_Similarity_body.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Views Similarity\n",
    "with given file of 2021-08 current views, Intersect the files in index with related views.\n",
    "Sort & Slice(N=30)\n",
    "Later on write to Disk as pkl OR find way to keep on memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pageViews import PageViews\n",
    "N = 30\n",
    "pageViews_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "docs_views = PageViews(pageViews_path, docs)\n",
    "print(sorted(list(docs_views.items()), key=lambda x: x[1], reverse=True)[:N])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33544886 0.86728093 0.52826982]\n",
      "[0.72895648 0.49157933 0.25559556]\n",
      "[0.64824571 0.        ]\n",
      "[0.]\n",
      "[0.60369555 1.17328514]\n",
      "[0.]\n",
      "[0.4024637  0.71822825 0.32968631]\n",
      "[0.70824051 0.26842282 0.25559556]\n",
      "[1.52996078]\n",
      "[0.4024637  0.53108087 0.71822825]\n",
      "[2.69875279]\n",
      "[1.17328514 0.89783141]\n",
      "[2.82369152]\n",
      "[0.38339334 0.68826675]\n",
      "[0.9998914  0.88466694]\n",
      "[2.69875279]\n",
      "[0.6525888  0.94291972]\n",
      "[0.83367217 1.12971005]\n",
      "[0.60369555 0.69616389]\n",
      "[2.82369152]\n",
      "[0.60369555 0.        ]\n",
      "[1.56841903]\n",
      "[1.06236077 0.        ]\n",
      "[0.52280634 0.4811601  0.72895648]\n",
      "[0.25559556 0.4588445  0.89958426]\n",
      "[1.3493764  1.26133077]\n",
      "[0.]\n",
      "[0.33544886 0.33917052 0.41463598]\n",
      "[0. 0.]\n",
      "[1.41184576 1.03518194]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best marvel movie': [['Howard Nostrand', 0.96515],\n",
       "  ['Ondine (ballet)', 0.82262],\n",
       "  ['Greek diacritics', 0.81094],\n",
       "  ['Viscoplasticity', 0.81094],\n",
       "  ['Modulating retro-reflector', 0.81094],\n",
       "  ['Jaguar 420 and Daimler Sovereign (1966–1969)', 0.58513],\n",
       "  ['Speaking in Strings', 0.58513],\n",
       "  ['Loosely Tight', 0.58513],\n",
       "  ['120 (film)', 0.58513],\n",
       "  ['EA Bright Light', 0.58513],\n",
       "  ['Tomoaki Maeno', 0.58513],\n",
       "  ['S. Darko', 0.58245],\n",
       "  ['Roberto Ampuero', 0.55201],\n",
       "  ['Tony Duquette', 0.55201],\n",
       "  ['Mannhai', 0.55201],\n",
       "  ['Ariel Winter', 0.55201],\n",
       "  ['Yellow Bird (company)', 0.52867],\n",
       "  ['Tatsuhisa Suzuki', 0.51722],\n",
       "  ['Nathan Furst', 0.51284],\n",
       "  ['Warehouse 13', 0.49395],\n",
       "  ['2008 California Proposition 4', 0.49395],\n",
       "  ['Conanicut Island Light', 0.49395],\n",
       "  ['Conimicut Light', 0.49395],\n",
       "  ['Warwick Light', 0.49395],\n",
       "  ['Sci-Fi on the Rock', 0.49395],\n",
       "  ['Buffalo Sharks', 0.49395],\n",
       "  ['Big Noise from Winnetka', 0.49395],\n",
       "  ['Kenny Wollesen', 0.49395],\n",
       "  ['John Hilliard', 0.49395],\n",
       "  ['List of University of Pittsburgh buildings', 0.49395]],\n",
       " 'How do kids come to world?': [['FAQs (film)', 0.9799],\n",
       "  ['Kerouac: Kicks Joy Darkness', 0.90544],\n",
       "  [\"1999 Kids' Choice Awards\", 0.82719],\n",
       "  ['Elias Kifle', 0.8076],\n",
       "  ['ABC Wide Bay', 0.79614],\n",
       "  ['List of people with the Korean family name Lee', 0.79614],\n",
       "  ['List of people with the Korean family name Kim', 0.79614],\n",
       "  ['Sport in Catalonia', 0.79614],\n",
       "  ['1965–66 Boston Celtics season', 0.79614],\n",
       "  ['First Student Canada', 0.79614],\n",
       "  ['1941 in the Soviet Union', 0.79614],\n",
       "  ['Steve Roslonek', 0.79614],\n",
       "  ['Takayuki Hamana', 0.79614],\n",
       "  ['Lee International', 0.60512],\n",
       "  ['Ondine (ballet)', 0.60512],\n",
       "  ['Big Noise from Winnetka', 0.60512],\n",
       "  ['Sarah Maria Cornell', 0.60512],\n",
       "  ['Political positions of Ronald Reagan', 0.60512],\n",
       "  ['2008–09 U.S. Città di Palermo season', 0.60512],\n",
       "  ['2008 Bolivian vote of confidence referendum', 0.60512],\n",
       "  ['Battle of the Beaufort (1982)', 0.60512],\n",
       "  ['Alan Hunte', 0.60512],\n",
       "  ['Edith Wilson (singer)', 0.60512],\n",
       "  ['Marguerite (musical)', 0.60428],\n",
       "  ['Percy Vear', 0.58985],\n",
       "  ['2008 Lebanon conflict', 0.57667],\n",
       "  ['Sid Kuller', 0.57335],\n",
       "  ['Old Man Logan', 0.57335],\n",
       "  ['100th Indiana Infantry Regiment', 0.57335],\n",
       "  ['Wood Brothers Racing', 0.54046]],\n",
       " 'Information retrieval': [['List of Scottish Football League clubs', 1.0],\n",
       "  ['Houstonia longifolia', 1.0],\n",
       "  ['No W', 1.0],\n",
       "  ['Lomita Park, California', 1.0],\n",
       "  ['Malverde (musician)', 1.0],\n",
       "  ['An Awfully Big Adventure', 1.0],\n",
       "  ['Canada Science and Technology Museum', 1.0],\n",
       "  ['Langenæs', 1.0],\n",
       "  ['International cricket in 2008–09', 1.0],\n",
       "  ['Korphe', 1.0],\n",
       "  ['Ming-Jun Lai', 1.0],\n",
       "  ['Cedarcroft, Baltimore', 1.0],\n",
       "  ['Bay Island', 1.0],\n",
       "  ['1999–2000 New Jersey Nets season', 1.0],\n",
       "  ['2002–03 New Jersey Nets season', 1.0],\n",
       "  ['GLITS', 1.0],\n",
       "  ['Great Observatories Origins Deep Survey', 1.0],\n",
       "  ['Physician Data Query', 1.0],\n",
       "  ['Filip Polášek', 1.0],\n",
       "  ['Margaret Woodbury Strong', 1.0],\n",
       "  ['Warehouse 13', 1.0],\n",
       "  ['Jacinto-class patrol vessel', 1.0],\n",
       "  ['HTC Touch Diamond', 1.0],\n",
       "  ['Incredible (Clique Girlz album)', 1.0],\n",
       "  ['House Office Building Commission', 1.0],\n",
       "  ['Kirill Yevstigneyev', 1.0],\n",
       "  ['1904–05 Southern Football League', 1.0],\n",
       "  ['Rob Kardashian', 1.0],\n",
       "  [\"2003 Men's European Water Polo Championship\", 1.0],\n",
       "  ['Torneo República', 1.0]],\n",
       " 'LinkedIn': [],\n",
       " 'How to make coffee?': [['List of United States Air Force air control squadrons',\n",
       "   1.0],\n",
       "  ['Empresas Públicas de Medellín', 1.0],\n",
       "  ['2008–09 in Scottish football', 0.8892],\n",
       "  ['The Irish Post', 0.8892],\n",
       "  ['Nightmare Creatures II', 0.8892],\n",
       "  ['Manggarai people', 0.8892],\n",
       "  ['Al-Muwaffaq', 0.8892],\n",
       "  ['Folger', 0.8892],\n",
       "  ['2000 United States presidential election in Alabama', 0.8892],\n",
       "  ['An Awfully Big Adventure', 0.45752],\n",
       "  ['Furryville', 0.45752],\n",
       "  ['Open textbook', 0.45752],\n",
       "  ['2008 Lebanon conflict', 0.45752],\n",
       "  ['Energy Resources Conservation Board', 0.45752],\n",
       "  ['Johnny Marr guest musician recordings', 0.45752],\n",
       "  ['Ralph C. Smith', 0.45752],\n",
       "  ['Modulating retro-reflector', 0.45752],\n",
       "  ['Extreme Justice (film)', 0.45752],\n",
       "  ['Sci-Fi on the Rock', 0.45752],\n",
       "  ['Nuit (song)', 0.45752],\n",
       "  ['A Gent from Bear Creek', 0.45752],\n",
       "  ['Hurricane Alma (1996)', 0.45752],\n",
       "  ['Roneat thung', 0.45752],\n",
       "  ['Mallu Magalhães', 0.45752],\n",
       "  ['Yushania alpina', 0.45752],\n",
       "  ['Collaborative Fusion', 0.45752],\n",
       "  ['Political positions of Ronald Reagan', 0.45752],\n",
       "  ['History of soccer in Newcastle, New South Wales', 0.45752],\n",
       "  ['FV300 Series', 0.45752],\n",
       "  ['James Ford Bell Lecture', 0.45752]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from search import Search\n",
    "from cosineSimilarity import Cosine_Similarity\n",
    "from binarySimilarity import Binary_Similarity\n",
    "\n",
    "queries_tokenize = {q_id: tokenize(q) for q_id, q in queries.items()}\n",
    "search = Search()\n",
    "tfidf_queries_score_train_cosS_body = search.get_topN_score_for_queries(queries_tokenize, body_idx, N=30, score=Cosine_Similarity)\n",
    "\n",
    "{queries[q_id] : list(map(get_page_titles, results)) for q_id, results in \\\n",
    "    list(tfidf_queries_score_train_cosS_body.items())[:5]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f657200a11c2a7a5d7b6c617421059e1795d7a954213580787bad10a8704a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
