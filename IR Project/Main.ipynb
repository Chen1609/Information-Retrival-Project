{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "\n",
    "pkl_file = \"part15_preprocessed.pkl\"\n",
    "try:\n",
    "    if os.environ[\"assignment_2_data\"] is not None:\n",
    "      pkl_file = Path(os.environ[\"assignment_2_data\"])\n",
    "except:\n",
    "   Exception(\"Problem with one of the variables\")\n",
    "   \n",
    "assert os.path.exists(pkl_file), 'You must upload this file.'\n",
    "with open(pkl_file, 'rb') as f:\n",
    "  pages = pickle.load(f)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from Parser import process_wiki\n",
    "all_words_body, all_words_title, all_words_anchor, docs = process_wiki(pages, 'all_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverted_index import InvertedIndex\n",
    "\n",
    "body_idx = InvertedIndex.read_index('body_indices', 'all_words')\n",
    "title_idx = InvertedIndex.read_index('title_index', 'all_words')\n",
    "anchor_idx = InvertedIndex.read_index('anchor_index', 'all_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eden\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "stopwords_frozen = frozenset(stopwords.words('english'))\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: string , represting the text to tokenize.    \n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    list of tokens (e.g., list of tokens).\n",
    "    \"\"\"\n",
    "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
    "    return list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('queries_train.json') as f:\n",
    "    data = json.load(f)\n",
    "# (q_id : (query_text, [relvent docs_ids]))\n",
    "test_data = {q_id: d for q_id, d in enumerate(data.items())}\n",
    "queries = {q_id: qf[0] for q_id, qf in test_data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import Search\n",
    "from cosineSimilarity import Cosine_Similarity\n",
    "from binarySimilarity import Binary_Similarity\n",
    "\n",
    "queries_tokenize = {q_id: tokenize(q) for q_id, q in queries.items()}\n",
    "search = Search()\n",
    "tfidf_queries_score_train = search.get_topN_score_for_queries(queries_tokenize, title_idx, N=30, score=Binary_Similarity)\n",
    "tfidf_queries_score_train = search.get_topN_score_for_queries(queries_tokenize, anchor_idx, N=30, score=Binary_Similarity)\n",
    "tfidf_queries_score_train = search.get_topN_score_for_queries(queries_tokenize, body_idx, N=30, score=Cosine_Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: [], 22: [], 23: [], 24: [], 25: [], 26: [], 27: [], 28: [], 29: []}\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for q_id, result in tfidf_queries_score_train.items():\n",
    "    results_files = list(map(lambda x: x[0], result))\n",
    "    results[q_id] = list(set(results_files) & set(test_data[q_id][1]))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17329265, 342890), (17349106, 114791), (17325260, 111797), (17333702, 59995), (17349121, 51330), (17328216, 43106), (17330081, 25231), (17325132, 24536), (17346957, 18890), (17345761, 18875), (17346443, 17744), (17348973, 16974), (17346464, 16862), (17346462, 15846), (17346456, 15036), (17337102, 14155), (17346283, 13825), (17346467, 13066), (17345999, 12783), (17331271, 12443), (17337527, 12148), (17346563, 11735), (17346433, 11660), (17333662, 8886), (17344572, 8884), (17326499, 8369), (17340975, 8280), (17327112, 8116), (17328064, 7254), (17341983, 6607)]\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from pageViews import PageViews\n",
    "N = 30\n",
    "pageViews_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
    "docs_views = PageViews(pageViews_path, docs)\n",
    "print(sorted(list(docs_views.items()), key=lambda x: x[1], reverse=True)[:N])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc7f657200a11c2a7a5d7b6c617421059e1795d7a954213580787bad10a8704a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
