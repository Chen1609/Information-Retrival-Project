{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTDWnKygoyeG",
        "outputId": "a5b5adc1-600d-463e-84f7-7f5e19858e10"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-c14d4ffe0494>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from collections import Counter, OrderedDict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from inverted_index import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3SiajkOdICW"
      },
      "source": [
        "### Tokenizer\n",
        "Problems:\n",
        "- its doesn't catch words the same if they have signs words != words?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEbl7cvG781q"
      },
      "outputs": [],
      "source": [
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "stopwords_frozen = frozenset(stopwords.words('english'))\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    text: string , represting the text to tokenize.    \n",
        "    \n",
        "    Returns:\n",
        "    -----------\n",
        "    list of tokens (e.g., list of tokens).\n",
        "    \"\"\"\n",
        "    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    \n",
        "    return list_of_tokens\n",
        "\n",
        "default_tokenizer = lambda text: tokenize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJpZDTSIJoVb",
        "outputId": "c7bf89b3-299b-4f2e-8d21-f51f2d8339d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'2022',\n",
              " 'Air',\n",
              " 'Apple',\n",
              " 'Ciggarets',\n",
              " 'Cup',\n",
              " 'Dolly',\n",
              " 'Elon',\n",
              " 'How',\n",
              " 'India',\n",
              " 'Information',\n",
              " 'Java',\n",
              " 'Jordan',\n",
              " 'Language',\n",
              " 'LinkedIn',\n",
              " 'Marijuana',\n",
              " 'Morty',\n",
              " 'Most',\n",
              " 'Natural',\n",
              " 'Netflix',\n",
              " 'Rick',\n",
              " 'Ritalin',\n",
              " 'Simpsons',\n",
              " 'The',\n",
              " 'What',\n",
              " 'Winter',\n",
              " 'World',\n",
              " 'and',\n",
              " 'at',\n",
              " 'best',\n",
              " 'breed',\n",
              " 'city',\n",
              " 'coffee',\n",
              " 'come',\n",
              " 'computer',\n",
              " 'cup',\n",
              " 'deal',\n",
              " 'depression',\n",
              " 'do',\n",
              " 'expensive',\n",
              " 'fast',\n",
              " 'flowers',\n",
              " 'gold',\n",
              " 'home',\n",
              " 'how',\n",
              " 'hummus',\n",
              " 'in',\n",
              " 'is',\n",
              " 'kids',\n",
              " 'live',\n",
              " 'lose',\n",
              " 'make',\n",
              " 'marvel',\n",
              " 'money',\n",
              " 'movie',\n",
              " 'musk',\n",
              " 'place',\n",
              " 'processing',\n",
              " 'retrieval',\n",
              " 'sheep',\n",
              " 'the',\n",
              " 'to',\n",
              " 'weight',\n",
              " 'wine',\n",
              " 'with',\n",
              " 'world',\n",
              " 'you'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from itertools import chain\n",
        "\n",
        "with open('queries_train.json') as f:\n",
        "    data = json.load(f)\n",
        "# (q_id : (query_text, [relvent docs_ids]))\n",
        "test_data = {q_id: d for q_id, d in enumerate(data.items())}\n",
        "queries = {q_id: qf[0] for q_id, qf in test_data.items()}\n",
        "queries_tokenize = {q_id: tokenize(q) for q_id, q in queries.items()}\n",
        "\n",
        "\n",
        "## Replace temporary solution with replace, to eliminate signs words != words? BETTER PARSER\n",
        "queries_words = set()\n",
        "for l in queries.values():\n",
        "  for w in l.split():\n",
        "    queries_words.add(w.replace('?', ''))\n",
        "\n",
        "queries_words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lFz5mndo5c7",
        "outputId": "b5f22c24-3afb-45d0-f2a9-adb268fcbc3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 36.6 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~18.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~18.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2023-01-11 17:33:30--  https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar\n",
            "Resolving repos.spark-packages.org (repos.spark-packages.org)... 108.156.83.37, 108.156.83.116, 108.156.83.69, ...\n",
            "Connecting to repos.spark-packages.org (repos.spark-packages.org)|108.156.83.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 247880 (242K) [binary/octet-stream]\n",
            "Saving to: ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’\n",
            "\n",
            "graphframes-0.8.2-s 100%[===================>] 242.07K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-01-11 17:33:31 (4.08 MB/s) - ‘/usr/local/lib/python3.7/dist-packages/pyspark/jars/graphframes-0.8.2-spark3.2-s_2.12.jar’ saved [247880/247880]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# These will already be installed in the testing environment so disregard the \n",
        "# amount of time (~1 minute) it takes to install. \n",
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "graphframes_jar = 'https://repos.spark-packages.org/graphframes/graphframes/0.8.2-spark3.2-s_2.12/graphframes-0.8.2-spark3.2-s_2.12.jar'\n",
        "spark_jars = '/usr/local/lib/python3.7/dist-packages/pyspark/jars'\n",
        "!wget -N -P $spark_jars $graphframes_jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vUN7NGVqo8Jj"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3bkUfW1go-AM"
      },
      "outputs": [],
      "source": [
        "# Initializing spark context\n",
        "# create a spark context and session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "conf.set(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "sc.addPyFile(str(Path(spark_jars) / Path(graphframes_jar).name))\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "FsvuPzpSo_Tg",
        "outputId": "6345f95d-f030-4744-b4ef-6ae477684995"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6a18285b6e1b:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f48cb4124c0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ho1enmQkpBwA"
      },
      "outputs": [],
      "source": [
        "# Authenticate your user\n",
        "# The authentication should be done with the email connected to your GCP account\n",
        "from google.colab import auth\n",
        "import signal\n",
        "\n",
        "AUTH_TIMEOUT = 30\n",
        "\n",
        "def handler(signum, frame):\n",
        "  raise Exception(\"Authentication timeout!\")\n",
        "\n",
        "try:\n",
        "  signal.signal(signal.SIGALRM, handler)\n",
        "  signal.alarm(AUTH_TIMEOUT)\n",
        "  auth.authenticate_user()\n",
        "  signal.alarm(0)\n",
        "except: \n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EueyWivJpC3A",
        "outputId": "caf4752a-3e08-419b-e56d-c1d0656f35f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Copying gs://wikidata20210801_preprocessed/multistream1_preprocessed.parquet...\n",
            "/ [1 files][316.7 MiB/316.7 MiB]                                                \n",
            "Operation completed over 1 objects/316.7 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "# Copy one wikidumps files \n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import auth\n",
        "## RENAME the project_id to yours project id from the project you created in GCP \n",
        "project_id = 'main-analog-370510'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "data_bucket_name = 'wikidata20210801_preprocessed'\n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "        pass  \n",
        "except:\n",
        "      !mkdir wikidumps\n",
        "      !gsutil -u {project_id} cp gs://{data_bucket_name}/multistream1_preprocessed.parquet \"wikidumps/\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bu_gQ2lvpEbj",
        "outputId": "d9ab973d-e1f1-4c44-e9cb-ebf53139d343"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+--------------------+--------------------+--------------------+\n",
            "| id|               title|                text|         anchor_text|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "| 12|           Anarchism|'''Anarchism''' i...|[{23040, politica...|\n",
            "| 25|              Autism|'''Autism''' is a...|[{492271, Clinica...|\n",
            "| 39|              Albedo|thumb|upright=1.3...|[{679294, diffuse...|\n",
            "|290|                   A|'''A''', or '''a'...|[{290, See below}...|\n",
            "|303|             Alabama|'''Alabama''' () ...|[{351590, Yellowh...|\n",
            "|305|            Achilles|thumb|260px|Ancie...|[{1076007, potter...|\n",
            "|307|     Abraham Lincoln|'''Abraham Lincol...|[{1827174, Alexan...|\n",
            "|308|           Aristotle|'''Aristotle''' (...|[{1389981, bust},...|\n",
            "|309|An American in Paris|'''''An American ...|[{13066, George G...|\n",
            "|316|Academy Award for...|The '''Academy Aw...|[{39842, Academy ...|\n",
            "|324|      Academy Awards|The '''Academy Aw...|[{649481, film in...|\n",
            "|330|             Actrius|'''''Actresses'''...|[{5282, Catalan},...|\n",
            "|332|     Animalia (book)|'''''Animalia''''...|[{2511084, Graeme...|\n",
            "|334|International Ato...|'''International ...|[{25453985, atomi...|\n",
            "|336|            Altruism|thumb|Giving alms...|[{657573, alms}, ...|\n",
            "|339|            Ayn Rand|'''Alice O'Connor...|[{24320051, St. P...|\n",
            "|340|        Alain Connes|'''Alain Connes''...|[{1201522, Dragui...|\n",
            "|344|          Allan Dwan|'''Allan Dwan''' ...|[{64646, Toronto}...|\n",
            "|358|             Algeria|'''Algeria''', of...|[{803, Arabic}, {...|\n",
            "|359|List of Atlas Shr...|This is a list of...|[{339, Ayn Rand},...|\n",
            "+---+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path \n",
        "import os\n",
        "\n",
        "try:\n",
        "    if os.environ[\"wikidata_preprocessed\"] is not None:\n",
        "      path = os.environ[\"wikidata_preprocessed\"]+\"/wikidumps/*\"\n",
        "except:\n",
        "      path = \"wikidumps/*\"\n",
        "\n",
        "parquetFile = spark.read.parquet(path)\n",
        "parquetFile.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aUAkNYmxs6EY"
      },
      "outputs": [],
      "source": [
        "doc_body_pairs = parquetFile.limit(1).select(\"id\", \"text\")\n",
        "doc_title_pairs = parquetFile.limit(1).select(\"id\", \"title\")\n",
        "doc_anchor_pairs = parquetFile.limit(1).select(\"id\", \"anchor_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9BcRrbLvMga",
        "outputId": "7e738138-ace7-4eb3-b7b6-32398cfe5ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n",
            "+---+--------------------+\n",
            "| id|         anchor_text|\n",
            "+---+--------------------+\n",
            "| 12|[{23040, politica...|\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "doc_title_pairs.printSchema()\n",
        "doc_body_pairs.printSchema()\n",
        "doc_anchor_pairs.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y0fo0pa49nWz"
      },
      "outputs": [],
      "source": [
        "doc_body_pairs_rdd = parquetFile.limit(1000).select(\"id\", \"text\").rdd\n",
        "doc_title_pairs_rdd = parquetFile.limit(1000).select(\"id\", \"title\").rdd\n",
        "doc_anchor_pairs_rdd = parquetFile.limit(1000).select(\"id\", \"anchor_text\").rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSGgXAE3Q5kH"
      },
      "source": [
        "### Tokenizer\n",
        "will change in the future do classify:\n",
        "- Bold\n",
        "- Dates\n",
        "- U.S.A == USA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px3vFZQmRG2R"
      },
      "source": [
        "#### Reset the file system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7t3NNFCtN3Zz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_wiki(pages, index_name='first', tokenize_func=default_tokenizer, words=None):\n",
        "  \"\"\" Process wikipedia: tokenize article body, title, anchor text, and create\n",
        "      indices for them. Each index is named `index_name` and placed in a \n",
        "      directory under the current dir named 'body_indices', 'title_index' \n",
        "      and 'anchor_index', respectively. \n",
        "  Parameters:\n",
        "  -----------\n",
        "  pages: list of tuples\n",
        "    Each tuple is a wiki article with id, title, body, and \n",
        "    [(target_article_id, anchor_text), ...]. \n",
        "  index_name: str\n",
        "    The name for the index.\n",
        "  tokenize_func: function str -> list of str\n",
        "    Tokenization function that takes text as input and return a list of \n",
        "    tokens.\n",
        "  Returns:\n",
        "  --------\n",
        "  Three inverted index objects\n",
        "    body_index, title_index, anchor_index.\n",
        "  \"\"\"\n",
        "  # create the index for titles\n",
        "  # collect anchor text tokens for each target article by its id\n",
        "\n",
        "  # iterate over batches of pages from the dump\n",
        "  # tokenize\n",
        "  # create a separate index of articles body for article in this batch\n",
        "  id2anchor_text = defaultdict(list)\n",
        "  body_index = InvertedIndex(data_set=words)\n",
        "  title_index = InvertedIndex(data_set=words)\n",
        "  anchor_index = InvertedIndex(data_set=words)\n",
        "  batch_idx, documents = pages\n",
        "\n",
        "  for id, title, body, anchor in documents:\n",
        "    body_index.add_doc(id, tokenize_func(body))\n",
        "    title_index.add_doc(id, tokenize_func(title))\n",
        "\n",
        "    for target_id, anchor_text in anchor:\n",
        "      anchor_index.add_doc(target_id, anchor_text)\n",
        "\n",
        "\n",
        "  body_index.write('./body_indices', f'{index_name}_{batch_idx}')\n",
        "  title_index.write('./title_index', f'{index_name}_{batch_idx}')\n",
        "  anchor_index.write('./anchor_index', f'{index_name}_{batch_idx}')\n",
        "\n",
        "  return f'{index_name}_{batch_idx}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFUYbqUeMAOb"
      },
      "outputs": [],
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "bucket_names = parquetFile.rdd.map(lambda document: (token2bucket_id(str(document[0])), list(document))).groupByKey().map(lambda pages: process_wiki(pages, words=queries_words)).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHJ67gbuFTKz"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree('./body_indices', ignore_errors=True)\n",
        "shutil.rmtree('./title_index', ignore_errors=True)\n",
        "shutil.rmtree('./anchor_index', ignore_errors=True)\n",
        "\n",
        "!mkdir body_indices title_index anchor_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BILoFaQ5GV8T"
      },
      "outputs": [],
      "source": [
        "# doc_body_pairs_rdd.map(lambda page: process_wiki(page, index_name='first', index_part = 'body_indices', tokenize_func=default_tokenizer))\n",
        "# doc_anchor_pairs_rdd.map(lambda page: process_wiki(page, index_name='first', index_part = 'anchor_index', tokenize_func=default_tokenizer))\n",
        "# doc_title_pairs_rdd.map(lambda page: process_wiki(page, index_name='first', index_part = 'title_index', tokenize_func=default_tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-PY6u9gRQE8"
      },
      "source": [
        "### Process wiki\n",
        "Here we process the wikipedia pages given. working on batchs sended (RDD) format (id, title, body, anchor)\n",
        "we build indexs for each batch:\n",
        "- title_index\n",
        "- text_index\n",
        "- anchor index\n",
        "and return the as list RDD contain \n",
        "f'{\n",
        "  index_name= name how you like,\n",
        "  _\n",
        "  batch_idx= bucket index\n",
        "}\n",
        "we hold all data in inverted index:\n",
        "- posting_lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDuyYTwNwSP-"
      },
      "outputs": [],
      "source": [
        "with open('buckets.txt', 'w') as f:\n",
        "    for bucket in bucket_names:\n",
        "        f.write(f\"{bucket}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJuBhqytZMxL"
      },
      "outputs": [],
      "source": [
        "with open(\"buckets\", \"rb\") as fp:\n",
        "  bucket_names = pickle.load(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9TN9TYPL0_q"
      },
      "outputs": [],
      "source": [
        "title_idx = InvertedIndex()\n",
        "title_idx.merge_indices('title_index', bucket_names, 'first')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9NV_PTJCXC_"
      },
      "outputs": [],
      "source": [
        "body_idx = InvertedIndex()\n",
        "body_idx.merge_indices('body_indices', bucket_names, 'first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxZSbPHtcxDI"
      },
      "outputs": [],
      "source": [
        "anchor_idx = InvertedIndex()\n",
        "body_idx.merge_indices('title_index', bucket_names, 'first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bJIaXOBEnt6"
      },
      "outputs": [],
      "source": [
        "# from search import Search\n",
        "# from cosineSimilarity import Cosine_Similarity\n",
        "\n",
        "# queries_tokenize = dict(list({q_id: tokenize(q) for q_id, q in queries.items()}.items())[0])\n",
        "# search = Search()\n",
        "# tfidf_queries_score_train_cosS_body = search.get_topN_score_for_queries(queries_tokenize, body_idx, N=30, score=Cosine_Similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRErunZzG_Dd"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "import json\n",
        "\n",
        "def get_page_titles(pagesId):\n",
        "  ''' Returns the title of the first, fourth, and fifth pages as ranked about \n",
        "      by PageRank.\n",
        "  Returns:\n",
        "  --------\n",
        "    list of three strings.\n",
        "  '''\n",
        "  # YOUR CODE HERE\n",
        "  def get_pageId_title(pageID):\n",
        "    url_json_wiki = lambda x : f\"https://en.wikipedia.org/w/api.php?action=query&inprop=url&pageids={x}&prop=info&format=json\"   \n",
        "    # store the response of URL\n",
        "    response = urlopen(url_json_wiki(pageID)) \n",
        "    # storing the JSON response \n",
        "    # from url in data\n",
        "    data_json = json.loads(response.read())\n",
        "    try:\n",
        "      return data_json['query']['pages'][str(pageID)]['title']\n",
        "    except:\n",
        "      return pageID\n",
        "    \n",
        "\n",
        "  titles = [get_pageId_title(pageId) for pageId in pagesId]\n",
        "  return titles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlQOMZAMmgdJ"
      },
      "outputs": [],
      "source": [
        "from search import Search\n",
        "from cosineSimilarity import Cosine_Similarity\n",
        "from binarySimilarity import Binary_Similarity\n",
        "\n",
        "queries_tokenize = {q_id: tokenize(q) for q_id, q in queries.items()}\n",
        "search = Search()\n",
        "tfidf_queries_score_train_bsS_title = search.get_topN_score_for_queries(queries_tokenize, title_idx, N=30, score=Binary_Similarity)\n",
        "# tfidf_queries_score_train_bsS_anchor = [search.get_topN_score_for_queries({q_id: query}, anchor_idx, N=30, score=Binary_Similarity) for q_id, query in queries_tokenize.items()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnL73YIOmn2B"
      },
      "outputs": [],
      "source": [
        "queries_results = {queries[q_id] : list(map(get_page_titles, results)) for q_id, results in tfidf_queries_score_train_bsS_title.items()}\n",
        "queries_results"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "dc7f657200a11c2a7a5d7b6c617421059e1795d7a954213580787bad10a8704a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
